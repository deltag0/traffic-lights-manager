DQN:
  4_way:
    lr: 0.001
    discount_factor: 0.99
    epsilon_decay: 0.9995
    min_epsilon: 0.05
    init_epsilon: 0.99
    model_version: dqn_1
    batch_size: 64  # size of batch of replayed memories
    max_len: 99999999  # max lenght of replay memories
    sync_steps: 1000  # number of steps until weights of policy network are copied to the target network
    episode_time: 10 # in seconds

PPO:
  4_way:
    lr: 0.001
    clip: 0.2
    discount_factor: 0.95
    model_version: ppo_1
    max_steps: 4  # number of steps before updating network (regular basis)
    overload: 4  # number of lanes to be filled for this to be considered a fail state
    batches: 2
    iteration_updates: 5  # number of time we perform on our actor
