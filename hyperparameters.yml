DQN:
  4_way:
    lr: 0.001
    discount_factor: 0.99
    epsilon_decay: 0.9995
    min_epsilon: 0.05
    init_epsilon: 0.99
    model_version: dqn_1
    batch_size: 64  # size of batch of replayed memories
    max_len: 99999999  # max lenght of replay memories
    sync_steps: 1000  # number of steps until weights of policy network are copied to the target network
    episode_time: 10 # in seconds

PPO:

  save_steps: 100

  4_way:
    lr: 0.0001
    clip: 0.2
    discount_factor: 0.98
    model_version: ppo_1
    max_steps: 99999  # max number of stps we allow model to run for
    update_steps: 40  # number of steps before updating network (regular basis)
    overload: 4  # number of lanes to be filled for this to be considered a fail state
    batches: 10
    iteration_updates: 5  # number of time we perform on our actor
    beta: 0.05  # for calculating the entropy
    overload_penalty: -1
    max_grad_norm: 0.5  # for clipping of grad norm
    lambda: 0.95

  2x2:
    lr: 0.001
    clip: 0.2
    discount_factor: 0.98
    model_version: ppo_3
    max_steps: 99999
    update_steps: 40
    overload: 64
    batches: 10
    iteration_updates: 5
    beta: 0.05
    overload_penalty: -1
    max_grad_norm: 0.5
    lambda: 0.95

  4x4:
    lr: 0.00008
    clip: 0.12
    discount_factor: 0.9
    model_version: ppo_6
    max_steps: 99999
    update_steps: 40
    overload: 64
    batches: 10
    iteration_updates: 5
    beta: 0.01
    overload_penalty: -1
    max_grad_norm: 0.5
    lambda: 0.9


Regular:
  4_way:
    max_steps: 99999
    test_version: test_1
